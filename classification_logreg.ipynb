{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "Seperating the attributes from the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>famhist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.057417</td>\n",
       "      <td>1.821099</td>\n",
       "      <td>0.477894</td>\n",
       "      <td>-0.295183</td>\n",
       "      <td>-0.418017</td>\n",
       "      <td>-0.176594</td>\n",
       "      <td>3.274189</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>1.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276789</td>\n",
       "      <td>-0.789382</td>\n",
       "      <td>-0.159507</td>\n",
       "      <td>0.411694</td>\n",
       "      <td>0.193134</td>\n",
       "      <td>0.670646</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>1.381617</td>\n",
       "      <td>-0.842361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.991731</td>\n",
       "      <td>-0.774141</td>\n",
       "      <td>-0.608585</td>\n",
       "      <td>0.883374</td>\n",
       "      <td>-0.112441</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>-0.540597</td>\n",
       "      <td>0.217947</td>\n",
       "      <td>1.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.545310</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.806252</td>\n",
       "      <td>1.622382</td>\n",
       "      <td>-0.214300</td>\n",
       "      <td>1.411091</td>\n",
       "      <td>0.294742</td>\n",
       "      <td>1.039361</td>\n",
       "      <td>1.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211103</td>\n",
       "      <td>2.169453</td>\n",
       "      <td>-0.598928</td>\n",
       "      <td>0.305020</td>\n",
       "      <td>0.702427</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>1.645991</td>\n",
       "      <td>0.423301</td>\n",
       "      <td>1.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>3.692037</td>\n",
       "      <td>-0.704470</td>\n",
       "      <td>0.598614</td>\n",
       "      <td>0.811401</td>\n",
       "      <td>1.109862</td>\n",
       "      <td>0.570971</td>\n",
       "      <td>-0.696228</td>\n",
       "      <td>1.039361</td>\n",
       "      <td>-0.842361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2.130781</td>\n",
       "      <td>0.122871</td>\n",
       "      <td>-0.159507</td>\n",
       "      <td>0.860240</td>\n",
       "      <td>-0.112441</td>\n",
       "      <td>0.608942</td>\n",
       "      <td>0.068445</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>-0.842361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-1.479624</td>\n",
       "      <td>-0.138395</td>\n",
       "      <td>-1.521228</td>\n",
       "      <td>-1.307946</td>\n",
       "      <td>-1.334744</td>\n",
       "      <td>-1.413043</td>\n",
       "      <td>0.391960</td>\n",
       "      <td>0.834008</td>\n",
       "      <td>-0.842361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>-0.991731</td>\n",
       "      <td>0.384137</td>\n",
       "      <td>3.317227</td>\n",
       "      <td>0.691875</td>\n",
       "      <td>1.109862</td>\n",
       "      <td>0.309916</td>\n",
       "      <td>0.282897</td>\n",
       "      <td>-0.192760</td>\n",
       "      <td>-0.842361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>-0.308682</td>\n",
       "      <td>-0.791559</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>1.028605</td>\n",
       "      <td>0.906144</td>\n",
       "      <td>-2.692210</td>\n",
       "      <td>-0.696228</td>\n",
       "      <td>0.217947</td>\n",
       "      <td>1.184570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "0    1.057417  1.821099  0.477894  -0.295183 -0.418017 -0.176594  3.274189   \n",
       "1    0.276789 -0.789382 -0.159507   0.411694  0.193134  0.670646 -0.612081   \n",
       "2   -0.991731 -0.774141 -0.608585   0.883374 -0.112441  0.734723 -0.540597   \n",
       "3    1.545310  0.841352  0.806252   1.622382 -0.214300  1.411091  0.294742   \n",
       "4   -0.211103  2.169453 -0.598928   0.305020  0.702427 -0.012842  1.645991   \n",
       "..        ...       ...       ...        ...       ...       ...       ...   \n",
       "457  3.692037 -0.704470  0.598614   0.811401  1.109862  0.570971 -0.696228   \n",
       "458  2.130781  0.122871 -0.159507   0.860240 -0.112441  0.608942  0.068445   \n",
       "459 -1.479624 -0.138395 -1.521228  -1.307946 -1.334744 -1.413043  0.391960   \n",
       "460 -0.991731  0.384137  3.317227   0.691875  1.109862  0.309916  0.282897   \n",
       "461 -0.308682 -0.791559  0.038474   1.028605  0.906144 -2.692210 -0.696228   \n",
       "\n",
       "          age   famhist  \n",
       "0    0.628654  1.184570  \n",
       "1    1.381617 -0.842361  \n",
       "2    0.217947  1.184570  \n",
       "3    1.039361  1.184570  \n",
       "4    0.423301  1.184570  \n",
       "..        ...       ...  \n",
       "457  1.039361 -0.842361  \n",
       "458  0.628654 -0.842361  \n",
       "459  0.834008 -0.842361  \n",
       "460 -0.192760 -0.842361  \n",
       "461  0.217947  1.184570  \n",
       "\n",
       "[462 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv(\"heart_data.txt\")\n",
    "\n",
    "cols = ['sbp', 'tobacco', 'ldl', 'adiposity', 'typea', 'obesity', 'alcohol', 'age']\n",
    "data = pd.DataFrame([file[col] for col in cols]).T\n",
    "famhist = [1 if val == 'Present' else 0 for val in file['famhist']]\n",
    "data['famhist'] = famhist\n",
    "data = pd.DataFrame(zscore(data, ddof=1))\n",
    "cols.append('famhist')\n",
    "data.columns = cols\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Reg Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(x_train: object, x_test: object, y_train: object, y_test: object):\n",
    "\n",
    "    logreg = LogisticRegression(solver='liblinear') #Defining solver\n",
    "    logreg.fit(x_train,y_train) #Modelfitting\n",
    "\n",
    "    pred_y_log = logreg.predict(x_test) \n",
    "\n",
    "    loss_function_log = nn.MSELoss()\n",
    "\n",
    "    loss_test_log = loss_function_log(pred_y_log.flatten(), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data seperation\n",
    "Seperating the attributes from the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_iterations(x_data: object, y_data: object, epochs: int, hidden_neurons: list, k: int = 10, ):\n",
    "    \n",
    "    k_inner = len(hidden_neurons) # Make as many k-folds as there are hidden neurons\n",
    "    \n",
    "    best_model = np.zeros(shape=k) # The number of hidden neurons for the best models will be stored here\n",
    "    \n",
    "    E_i_NN, final_train_error = np.zeros(shape=k), np.zeros(shape=k) # Error for best model in each k-fold will be stored here\n",
    "    validation_error = np.zeros(shape=(k_inner, k_inner)) # Error for each inner layer will be stored here\n",
    "    \n",
    "    splits_outer = KFold(n_splits = k, shuffle=True, random_state=42) # Defining outer splits\n",
    "    \n",
    "    for i, (train_idx_outer, test_idx_outer) in enumerate(splits_outer.split(x_data)): # Outer k-fold layer\n",
    "        \n",
    "        # Getting \"new\" data set for inner k-folds\n",
    "        x_train_outer = x_data.iloc[train_idx_outer]\n",
    "        y_train_outer = y_data.iloc[train_idx_outer]\n",
    "        \n",
    "        \n",
    "        splits_inner = KFold(n_splits = k_inner, shuffle=True, random_state=42) # Defining inner splits\n",
    "        \n",
    "        for j, (train_idx_inner, test_idx_inner) in enumerate(splits_inner.split(x_train_outer)): # Inner k-fold layer\n",
    "            \n",
    "            for jj, h in enumerate(hidden_neurons):\n",
    "                # Training models on inner k-folds\n",
    "                _, test_loss = ANN_model(epochs, \n",
    "                                        h,\n",
    "                                        x_train_outer, \n",
    "                                        y_train_outer,\n",
    "                                        train_idx_inner,\n",
    "                                        test_idx_inner\n",
    "                                        )\n",
    "            \n",
    "                # We're only interested in the last test error, \n",
    "                # as this defines the total loss for the given model\n",
    "                validation_error[j, jj] = test_loss[-1] \n",
    "            \n",
    "                '''\n",
    "                Put a function here that runs the linear regression or logistic regression\n",
    "                for inner k-folds and find the best lambda value for each outer k-fold\n",
    "                '''\n",
    "        \n",
    "        length_ratio = len(x_train_outer) / len(x_data)\n",
    "        generalization_error = [np.sum(length_ratio * np.array(validation_error_s)) for validation_error_s in validation_error]\n",
    "        \n",
    "        # Run model again for outer layer with best h-value from inner layers\n",
    "        h_best = hidden_neurons[np.argmin(generalization_error)]\n",
    "        train_loss, test_loss = ANN_model(epochs, \n",
    "                                 h_best,\n",
    "                                 x_data, \n",
    "                                 y_data,\n",
    "                                 train_idx_outer,\n",
    "                                 test_idx_outer\n",
    "                                )\n",
    "        \n",
    "        '''\n",
    "        Again put the linear or logistic regression model here \n",
    "        but this time evaluate on the outer k-fold\n",
    "        '''\n",
    "        \n",
    "        best_model[i] = h_best\n",
    "        E_i_NN[i] = test_loss[-1] # Appending last value for test loss in outer layer, this value will go in the final table\n",
    "        final_train_error[i] = train_loss[-1]\n",
    "    \n",
    "    return final_train_error, E_i_NN, best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
